{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "Get pokemon data from pokeAPI",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 定义保存图片的目录\n",
    "save_directory = \"data/pokemon_images\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# 获取所有宝可梦的基本信息\n",
    "url = \"https://pokeapi.co/api/v2/pokemon?limit=10000\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# 创建一个字典来存储宝可梦的名称和属性\n",
    "pokemon_info = {}\n",
    "\n",
    "# 遍历每个宝可梦\n",
    "for pokemon in data['results']:\n",
    "    pokemon_name = pokemon['name']\n",
    "    pokemon_url = pokemon['url']\n",
    "    response = requests.get(pokemon_url)\n",
    "    pokemon_data = response.json()\n",
    "\n",
    "    # 获取宝可梦的属性\n",
    "    types = pokemon_data['types']\n",
    "    type_names = [type_info['type']['name'] for type_info in types]\n",
    "\n",
    "    # 获取宝可梦的图片链接\n",
    "    pic1_url = pokemon_data['sprites']['other']['official-artwork']['front_default']\n",
    "    pic2_url = pokemon_data['sprites']['other']['home']['front_default']\n",
    "    pic3_url = pokemon_data['sprites']['front_default']\n",
    "\n",
    "    # 下载并保存图片\n",
    "    for i, pic_url in enumerate([pic1_url, pic2_url, pic3_url], start=1):\n",
    "        if pic_url:\n",
    "            img_response = requests.get(pic_url)\n",
    "            if img_response.status_code == 200:\n",
    "                img_name = f\"{pokemon_name}_pic{i}.png\"\n",
    "                img_path = os.path.join(save_directory, img_name)\n",
    "                with open(img_path, 'wb') as file:\n",
    "                    file.write(img_response.content)\n",
    "\n",
    "    # 将宝可梦的名称和属性存储到字典中\n",
    "    pokemon_info[pokemon_name] = {\n",
    "        \"types\": type_names,\n",
    "        \"images\": [os.path.join(save_directory, f\"{pokemon_name}_pic{i}.png\") for i in range(1, 4) if eval(f\"pic{i}_url\")]\n",
    "    }\n",
    "\n",
    "# 将字典转换为 JSON 格式并保存到文件中\n",
    "with open('data/pokemon_info.json', 'w') as json_file:\n",
    "    json.dump(pokemon_info, json_file, indent=4)\n",
    "\n",
    "print(f\"所有宝可梦的名称和属性已保存到 pokemon_info.json 文件中，图片保存在 {save_directory} 目录中。\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-07T08:33:30.775191Z",
     "iopub.execute_input": "2024-12-07T08:33:30.775637Z",
     "iopub.status.idle": "2024-12-07T08:45:52.057371Z",
     "shell.execute_reply.started": "2024-12-07T08:33:30.775598Z",
     "shell.execute_reply": "2024-12-07T08:45:52.056120Z"
    },
    "ExecuteTime": {
     "end_time": "2024-12-07T11:10:54.846644Z",
     "start_time": "2024-12-07T10:28:55.014281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有宝可梦的名称和属性已保存到 pokemon_info.json 文件中，图片保存在 data/pokemon_images 目录中。\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T03:27:45.300526Z",
     "start_time": "2024-12-11T03:27:45.288261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "\n",
    "# 定义自定义数据集类\n",
    "class PokemonDataset(Dataset):\n",
    "    def __init__(self, data_dict, transform=None):\n",
    "        self.data_dict = data_dict\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # 遍历数据字典，获取所有图像文件路径和对应的标签（属性）\n",
    "        for pokemon_name, info in data_dict.items():\n",
    "            for img_path in info['images']:\n",
    "                self.image_files.append(img_path)\n",
    "                self.labels.append(info['types'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # 获取宝可梦的标签（属性）\n",
    "        labels = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # 将字符串标签转换为整数标签\n",
    "        label_map = {'grass': 0, 'poison': 1, 'fire': 2, 'water': 3, 'electric': 4, 'ice': 5, 'fighting': 6,\n",
    "                     'ground': 7, 'flying': 8, 'psychic': 9, 'bug': 10, 'rock': 11, 'ghost': 12, 'dark': 13,\n",
    "                     'dragon': 14, 'steel': 15, 'fairy': 16, 'normal': 17}\n",
    "        labels = [label_map[label] for label in labels]\n",
    "        \n",
    "        # 将标签转换为多标签二进制格式\n",
    "        multi_label = torch.zeros(len(label_map), dtype=torch.float32)\n",
    "        for label in labels:\n",
    "            multi_label[label] = 1.0\n",
    "        \n",
    "        return image, multi_label\n",
    "\n",
    "# 读取数据字典\n",
    "with open('data/pokemon_info.json', 'r') as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "# 定义训练数据的转换\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 创建数据集\n",
    "pokemon_dataset = PokemonDataset(data_dict=data_dict, transform=data_transforms)\n",
    "\n",
    "# 将数据集分割为训练集、验证集和测试集（80%、10%、10%）\n",
    "train_size = int(0.8 * len(pokemon_dataset))\n",
    "val_size = int(0.1 * len(pokemon_dataset))\n",
    "test_size = len(pokemon_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(pokemon_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T02:22:56.600633Z",
     "start_time": "2024-12-12T02:18:49.221074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 定义自定义数据集类\n",
    "class PokemonDataset(Dataset):\n",
    "    def __init__(self, data_dict, transform=None):\n",
    "        self.data_dict = data_dict\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # 遍历数据字典，获取所有图像文件路径和对应的标签（属性）\n",
    "        for pokemon_name, info in data_dict.items():\n",
    "            for img_path in info['images']:\n",
    "                self.image_files.append(img_path)\n",
    "                self.labels.append(info['types'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        \n",
    "        # 使用OpenCV加载图像\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            print(f\"UnidentifiedImageError: cannot identify image file '{img_path}'\")\n",
    "            return self.__getitem__((idx + 1) % len(self.image_files))\n",
    "        \n",
    "        # 将图像从BGR转换为RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 获取宝可梦的标签（属性）\n",
    "        labels = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # 将字符串标签转换为整数标签\n",
    "        label_map = {'grass': 0, 'poison': 1, 'fire': 2, 'water': 3, 'electric': 4, 'ice': 5, 'fighting': 6,\n",
    "                     'ground': 7, 'flying': 8, 'psychic': 9, 'bug': 10, 'rock': 11, 'ghost': 12, 'dark': 13,\n",
    "                     'dragon': 14, 'steel': 15, 'fairy': 16, 'normal': 17}\n",
    "        labels = [label_map[label] for label in labels]\n",
    "        \n",
    "        # 将标签转换为多标签二进制格式\n",
    "        multi_label = torch.zeros(len(label_map), dtype=torch.float32)\n",
    "        for label in labels:\n",
    "            multi_label[label] = 1.0\n",
    "        \n",
    "        return image, multi_label\n",
    "\n",
    "# 定义函数来提取图像特征\n",
    "def extract_features(model, dataloader):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            features.append(outputs.cpu().numpy())\n",
    "    return np.concatenate(features)\n",
    "\n",
    "\n",
    "# 读取数据字典\n",
    "with open('data/pokemon_info.json', 'r') as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "# 定义训练数据的转换\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 创建数据集\n",
    "pokemon_dataset = PokemonDataset(data_dict=data_dict, transform=data_transforms)\n",
    "\n",
    "# 将数据集分割为训练集、验证集和测试集（80%、10%、10%）\n",
    "train_size = int(0.8 * len(pokemon_dataset))\n",
    "val_size = int(0.1 * len(pokemon_dataset))\n",
    "test_size = len(pokemon_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(pokemon_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 加载预训练的ResNet18模型\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# 修改最后一层以匹配属性的数量（假设有18种可能的属性）\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 18)\n",
    "\n",
    "# 将模型移动到GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# 训练循环\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs_labels in train_loader:\n",
    "        if inputs_labels is None:\n",
    "            continue\n",
    "        \n",
    "        inputs, labels = inputs_labels\n",
    "        \n",
    "        # 将输入和标签移动到GPU\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # 验证循环\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs_labels in val_loader:\n",
    "            if inputs_labels is None:\n",
    "                continue\n",
    "            \n",
    "            inputs, labels = inputs_labels\n",
    "            \n",
    "            # 将输入和标签移动到GPU\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# 测试循环（可选）\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs_labels in test_loader:\n",
    "        if inputs_labels is None:\n",
    "            continue\n",
    "        \n",
    "        inputs, labels = inputs_labels\n",
    "        \n",
    "        # 将输入和标签移动到GPU\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch 1/10, Loss: 0.30217974563491157\n",
      "Validation Loss: 0.28418717986529635\n",
      "Epoch 2/10, Loss: 0.2739431640721973\n",
      "Validation Loss: 0.2694471507650061\n",
      "Epoch 3/10, Loss: 0.26146089968903696\n",
      "Validation Loss: 0.2585177209573923\n",
      "Epoch 4/10, Loss: 0.2513724207839509\n",
      "Validation Loss: 0.24945543199470363\n",
      "Epoch 5/10, Loss: 0.2423582907881916\n",
      "Validation Loss: 0.24683019611024365\n",
      "Epoch 6/10, Loss: 0.23483559973730941\n",
      "Validation Loss: 0.23940254087300644\n",
      "Epoch 7/10, Loss: 0.22669326286241798\n",
      "Validation Loss: 0.2360087030941678\n",
      "Epoch 8/10, Loss: 0.21843267285260204\n",
      "Validation Loss: 0.22845934024176648\n",
      "Epoch 9/10, Loss: 0.2098351487976745\n",
      "Validation Loss: 0.22521687905813\n",
      "Epoch 10/10, Loss: 0.20106783593218253\n",
      "Validation Loss: 0.22107784250347884\n",
      "Training complete.\n",
      "Test Loss: 0.22897303227296809\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T02:28:43.307690Z",
     "start_time": "2024-12-12T02:28:24.592049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 示例：查找与查询图像最相似的三个宝可梦图像\n",
    "# 定义函数来找到最相似的宝可梦图像\n",
    "def find_similar_images(query_image_path, model, feature_extractor, dataset, dataloader, top_k=3):\n",
    "    # 加载查询图像并进行预处理\n",
    "    query_image = cv2.imread(query_image_path)\n",
    "    if query_image is None:\n",
    "        raise ValueError(f\"Cannot identify image file '{query_image_path}'\")\n",
    "    \n",
    "    query_image = cv2.cvtColor(query_image, cv2.COLOR_BGR2RGB)\n",
    "    query_image = data_transforms(query_image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 提取查询图像的特征\n",
    "    with torch.no_grad():\n",
    "        query_features = feature_extractor(query_image).cpu().numpy()\n",
    "    \n",
    "    # 提取数据集中所有图像的特征\n",
    "    dataset_features = extract_features(feature_extractor, dataloader)\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    similarities = cosine_similarity(query_features, dataset_features)\n",
    "    \n",
    "    # 找到最相似的图像索引\n",
    "    top_k_indices = np.argsort(similarities[0])[::-1][:top_k]\n",
    "    \n",
    "    # 返回最相似的图像路径和相似度分数\n",
    "    similar_images = [(dataset.image_files[idx], similarities[0][idx]) for idx in top_k_indices]\n",
    "    \n",
    "    return similar_images\n",
    "\n",
    "query_image_path = 'data/palworld_images/Anubis.png'\n",
    "similar_images = find_similar_images(query_image_path, model, model, pokemon_dataset, train_loader, top_k=3)\n",
    "print(\"Most similar images:\")\n",
    "for img_path, score in similar_images:\n",
    "    print(f\"Image: {img_path}, Similarity Score: {score}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar images:\n",
      "Image: data/pokemon_images\\passimian_pic3.png, Similarity Score: 0.9777513146400452\n",
      "Image: data/pokemon_images\\incineroar_pic1.png, Similarity Score: 0.9698570966720581\n",
      "Image: data/pokemon_images\\azurill_pic1.png, Similarity Score: 0.969271719455719\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T06:45:16.542150Z",
     "start_time": "2024-12-12T06:44:21.184900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def find_similar_images_euclidean(query_image_path, model, feature_extractor, dataset, dataloader, top_k=3):\n",
    "    query_image = cv2.imread(query_image_path)\n",
    "    if query_image is None:\n",
    "        raise ValueError(f\"Cannot identify image file '{query_image_path}'\")\n",
    "\n",
    "    query_image = cv2.cvtColor(query_image, cv2.COLOR_BGR2RGB)\n",
    "    query_image = data_transforms(query_image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        query_features = feature_extractor(query_image).cpu().numpy()\n",
    "\n",
    "    dataset_features = extract_features(feature_extractor, dataloader)\n",
    "\n",
    "    distances = euclidean_distances(query_features, dataset_features)\n",
    "\n",
    "    top_k_indices = np.argsort(distances[0])[:top_k]\n",
    "\n",
    "    similar_images = [(dataset.image_files[idx], distances[0][idx]) for idx in top_k_indices]\n",
    "\n",
    "    return similar_images\n",
    "\n",
    "query_image_path = 'data/palworld_images/Anubis.png'\n",
    "similar_images = find_similar_images_euclidean(query_image_path, model, model, pokemon_dataset, train_loader, top_k=3)\n",
    "print(\"Most similar images:\")\n",
    "for img_path, score in similar_images:\n",
    "    print(f\"Image: {img_path}, Distance: {score}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar images:\n",
      "Image: data/pokemon_images\\cetoddle_pic3.png, Distance: 3.8211874961853027\n",
      "Image: data/pokemon_images\\bastiodon_pic1.png, Distance: 3.8765923976898193\n",
      "Image: data/pokemon_images\\wyrdeer_pic1.png, Distance: 4.010998249053955\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T06:49:18.409588Z",
     "start_time": "2024-12-12T06:48:54.091301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.models import vgg16\n",
    "\n",
    "# 加载预训练的VGG16模型\n",
    "vgg_model = vgg16(pretrained=True)\n",
    "vgg_model.classifier = nn.Sequential(*list(vgg_model.classifier.children())[:-1])  # 移除最后一层\n",
    "vgg_model = vgg_model.to(device)\n",
    "\n",
    "def extract_vgg_features(model, dataloader):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            features.append(outputs.cpu().numpy())\n",
    "    return np.concatenate(features)\n",
    "\n",
    "query_image_path = 'data/palworld_images/Anubis.png'\n",
    "similar_images = find_similar_images(query_image_path, vgg_model, vgg_model, pokemon_dataset, train_loader, top_k=3)\n",
    "print(\"Most similar images:\")\n",
    "for img_path, score in similar_images:\n",
    "    print(f\"Image: {img_path}, Similarity Score: {score}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar images:\n",
      "Image: data/pokemon_images\\cradily_pic3.png, Similarity Score: 0.514254093170166\n",
      "Image: data/pokemon_images\\absol_pic2.png, Similarity Score: 0.49518799781799316\n",
      "Image: data/pokemon_images\\drowzee_pic2.png, Similarity Score: 0.4846184253692627\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T06:56:04.238928Z",
     "start_time": "2024-12-12T06:55:50.646301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import imagehash\n",
    "import numpy as np\n",
    " \n",
    "def find_similar_images_hash(query_image_path, dataset, top_k=3):\n",
    "    # 使用 OpenCV 读取图像\n",
    "    query_image = cv2.imread(query_image_path)\n",
    "    # OpenCV 读取的图像是 BGR 格式，需要转换为 RGB 格式\n",
    "    query_image_rgb = cv2.cvtColor(query_image, cv2.COLOR_BGR2RGB)\n",
    "    # 将图像转换为 Pillow 图像对象以使用 imagehash 库\n",
    "    query_image_pil = Image.fromarray(query_image_rgb)\n",
    "    query_hash = imagehash.phash(query_image_pil)\n",
    " \n",
    "    image_hashes = []\n",
    "    for img_path in dataset.image_files:\n",
    "        # 使用 OpenCV 读取图像\n",
    "        img = cv2.imread(img_path)\n",
    "        # 转换为 RGB 格式\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # 转换为 Pillow 图像对象\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        # 计算图像哈希\n",
    "        img_hash = imagehash.phash(img_pil)\n",
    "        image_hashes.append((img_path, img_hash))\n",
    " \n",
    "    similarities = [(img_path, 1 - (query_hash - img_hash) / len(query_hash.hash)**2) for img_path, img_hash in image_hashes]\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    " \n",
    "    return similarities[:top_k]\n",
    " \n",
    "query_image_path = 'data/palworld_images/Anubis.png'\n",
    "# 假设 pokemon_dataset 是一个具有 image_files 属性的对象，该属性包含图像文件路径的列表\n",
    "# 注意：你需要确保 pokemon_dataset 已经被正确定义并包含有效的图像文件路径\n",
    "similar_images = find_similar_images_hash(query_image_path, pokemon_dataset, top_k=3)\n",
    "print(\"Most similar images:\")\n",
    "for img_path, score in similar_images:\n",
    "    print(f\"Image: {img_path}, Similarity Score: {score}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar images:\n",
      "Image: data/pokemon_images\\pichu_pic3.png, Similarity Score: 0.75\n",
      "Image: data/pokemon_images\\chinchou_pic3.png, Similarity Score: 0.71875\n",
      "Image: data/pokemon_images\\porygon2_pic2.png, Similarity Score: 0.71875\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ]
}
