{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<font size=\"6\">**Task 1: data mining**</font>\n",
    "\n",
    "**1.1 Get pokemon data from pokeAPI**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 定义保存图片的目录\n",
    "save_directory = \"data/pokemon_images\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# 获取所有宝可梦的基本信息\n",
    "url = \"https://pokeapi.co/api/v2/pokemon?limit=10000\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# 创建一个字典来存储宝可梦的名称和属性\n",
    "pokemon_info = {}\n",
    "\n",
    "# 遍历每个宝可梦\n",
    "for pokemon in data['results']:\n",
    "    pokemon_name = pokemon['name']\n",
    "    pokemon_url = pokemon['url']\n",
    "    response = requests.get(pokemon_url)\n",
    "    pokemon_data = response.json()\n",
    "\n",
    "    # 获取宝可梦的属性\n",
    "    types = pokemon_data['types']\n",
    "    type_names = [type_info['type']['name'] for type_info in types]\n",
    "\n",
    "    # 获取宝可梦的图片链接\n",
    "    pic1_url = pokemon_data['sprites']['other']['official-artwork']['front_default']\n",
    "    pic2_url = pokemon_data['sprites']['other']['home']['front_default']\n",
    "    pic3_url = pokemon_data['sprites']['front_default']\n",
    "\n",
    "    # 下载并保存图片\n",
    "    for i, pic_url in enumerate([pic1_url, pic2_url, pic3_url], start=1):\n",
    "        if pic_url:\n",
    "            img_response = requests.get(pic_url)\n",
    "            if img_response.status_code == 200:\n",
    "                img_name = f\"{pokemon_name}_pic{i}.png\"\n",
    "                img_path = os.path.join(save_directory, img_name)\n",
    "                with open(img_path, 'wb') as file:\n",
    "                    file.write(img_response.content)\n",
    "\n",
    "    # 将宝可梦的名称和属性存储到字典中\n",
    "    pokemon_info[pokemon_name] = {\n",
    "        \"types\": type_names,\n",
    "        \"images\": [os.path.join(save_directory, f\"{pokemon_name}_pic{i}.png\") for i in range(1, 4) if eval(f\"pic{i}_url\")]\n",
    "    }\n",
    "\n",
    "# 将字典转换为 JSON 格式并保存到文件中\n",
    "with open('data/pokemon_info.json', 'w') as json_file:\n",
    "    json.dump(pokemon_info, json_file, indent=4)\n",
    "\n",
    "print(f\"所有宝可梦的名称和属性已保存到 pokemon_info.json 文件中，图片保存在 {save_directory} 目录中。\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-07T08:33:30.775191Z",
     "iopub.execute_input": "2024-12-07T08:33:30.775637Z",
     "iopub.status.idle": "2024-12-07T08:45:52.057371Z",
     "shell.execute_reply.started": "2024-12-07T08:33:30.775598Z",
     "shell.execute_reply": "2024-12-07T08:45:52.056120Z"
    },
    "ExecuteTime": {
     "end_time": "2024-12-07T11:10:54.846644Z",
     "start_time": "2024-12-07T10:28:55.014281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有宝可梦的名称和属性已保存到 pokemon_info.json 文件中，图片保存在 data/pokemon_images 目录中。\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**1.2 Palworlds**"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T08:19:18.970288Z",
     "start_time": "2024-12-13T08:19:16.218770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "url = 'https://palworld.wiki.gg/wiki/Pals'\n",
    "response = requests.get(url)\n",
    "names = []\n",
    "numbers = []\n",
    "elements = []\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    for table in tables:\n",
    "        tbody = table.find('tbody')\n",
    "        if tbody:\n",
    "            trs = tbody.find_all('tr')\n",
    "            for tr in trs[1:]:\n",
    "                name_a_tag = tr.find('span', class_='iconlink').find_all('a')[1]\n",
    "                name = name_a_tag.get_text(strip=True)\n",
    "                number = tr.find_all('td')[1].get_text(strip=True)\n",
    "                ele = []\n",
    "                for span_tag in tr.find_all('span'):\n",
    "                    if span_tag.name == 'span' and \"ele-\" in ' '.join(span_tag.get(\"class\", [])):\n",
    "                        element_a_tag = span_tag.find('a')\n",
    "                        element = element_a_tag.get_text(strip=True) if element_a_tag else \"Unknown\"\n",
    "                        ele.append(element)\n",
    "                element = ', '.join(ele) if ele else \"Unknown\"\n",
    "                names.append(name)\n",
    "                numbers.append(number)\n",
    "                elements.append(element)\n",
    "            df = pd.DataFrame({'No.': numbers,\n",
    "                               'Name': names,\n",
    "                               'Elements': elements})\n",
    "            csv_file_path = 'pals_data.csv'\n",
    "            df.to_csv(csv_file_path, index=False) "
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def download_image(image_url, image_name, folder=\"images\"):\n",
    "    image_path = os.path.join(folder, image_name)\n",
    "    response = requests.get(image_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"download：{image_path}\")\n",
    "    else:\n",
    "        print(f\"error：{response.status_code}\")\n",
    "def extract_image_urls(soup):\n",
    "    image_urls = []\n",
    "\n",
    "    screenshot_div = soup.find('div', id='pi-tab-1')\n",
    "    if screenshot_div:\n",
    "        screenshot_img = screenshot_div.find('img')\n",
    "        if screenshot_img:\n",
    "            image_urls.append(screenshot_img['src'])\n",
    "\n",
    "    icon_div = soup.find('div', id='pi-tab-2')\n",
    "    if icon_div:\n",
    "        icon_img = icon_div.find('img')\n",
    "        if icon_img:\n",
    "            image_urls.append(icon_img['src'])\n",
    "\n",
    "    return image_urls\n",
    "\n",
    "data=pd.read_csv('pals_data.csv')\n",
    "# names = data['Name'].tolist()\n",
    "index = data.index[data['Name'] == 'Petallia'].tolist()\n",
    "names = data['Name'].iloc[index[0]:].tolist()\n",
    "for name in names:\n",
    "    name=name.replace(' ','_')\n",
    "    url=f'https://palworld.wiki.gg/wiki/{name}'\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    image_urls = extract_image_urls(soup)\n",
    "    for i, image_url in enumerate(image_urls):\n",
    "        if 'icon' in image_url:\n",
    "            image_name = f'{name}_icon.png'\n",
    "        else:\n",
    "            image_name = f'{name}.png'\n",
    "        \n",
    "        image_url=f'https://palworld.wiki.gg{image_url}'\n",
    "        download_image(image_url, image_name)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**1.3 roco kingdom**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request\n",
    "url = 'https://rocokingdom.huijiwiki.com/wiki/%E5%AE%A0%E7%89%A9%E5%9B%BE%E9%89%B4'\n",
    "\n",
    "def open_driver(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging']) # for ignore warning and error\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "    return driver\n",
    "\n",
    "year=2014\n",
    "i=year-2008\n",
    "\n",
    "driver = open_driver(url)\n",
    "per_year_button = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.XPATH,f'/html/body/div[2]/div/div[1]/main/article/section[1]/div/div/div/div[2]/div/div/div[{i}]'))\n",
    ")\n",
    "per_year_button.click()\n",
    "\n",
    "WebDriverWait(driver, 30).until(\n",
    "    EC.invisibility_of_element_located((By.XPATH, \"//div[@class='pet-name' and text()='苦行骆驼']\")))\n",
    "\n",
    "\n",
    "time.sleep(10)\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.CLASS_NAME, \"pet-card\")))\n",
    "# time.sleep(10)\n",
    "\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "pet_cards = soup.find_all(class_=\"pet-card\")\n",
    "\n",
    "names = []\n",
    "numbers = []\n",
    "elements = []\n",
    "for pet_card in pet_cards:\n",
    "    number = pet_card.find('div', class_='pet-title huiji-tt').get('data-name')\n",
    "    name = pet_card.find('div', class_='pet-name').text\n",
    "    element = [img.get('title') for img in pet_card.find_all('img')]\n",
    "    element = element[1:len(element)-1]\n",
    "    element = [x for x in element if x is not None]\n",
    "    element = '，'.join(element)\n",
    "    names.append(name)\n",
    "    numbers.append(number)\n",
    "    elements.append(element)\n",
    "\n",
    "df = pd.DataFrame({'No.': numbers,\n",
    "                    'Name': names,\n",
    "                    'Elements': elements })\n",
    "    \n",
    "    # encoding problem\n",
    "    # csv_file_path = 'roco_kingdom_data_2010.csv'\n",
    "    # df.to_csv(csv_file_path, index=False, encoding='utf-8') \n",
    "\n",
    "txt_file_path = f'roco_kingdom_data_{year}.txt'\n",
    "df.to_csv(txt_file_path, index=False, sep='\\t', encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import urllib.request\n",
    "year=2014\n",
    "i=year-2008\n",
    "\n",
    "url = 'https://rocokingdom.huijiwiki.com/wiki/%E5%AE%A0%E7%89%A9%E5%9B%BE%E9%89%B4'\n",
    "\n",
    "# Terminal: /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222\n",
    "def create_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_experimental_option(\"debuggerAddress\", \"127.0.0.1:9222\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "driver = create_driver()\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')]\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "per_year_button = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.XPATH,f'/html/body/div[2]/div/div[1]/main/article/section[1]/div/div/div/div[2]/div/div/div[{i}]'))\n",
    ")\n",
    "per_year_button.click()\n",
    "\n",
    "WebDriverWait(driver, 30).until(\n",
    "    EC.invisibility_of_element_located((By.XPATH, \"//div[@class='pet-name' and text()='苦行骆驼']\")))\n",
    "\n",
    "time.sleep(10)\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.CLASS_NAME, \"pet-card\")))\n",
    "\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "pet_cards = soup.find_all(class_=\"pet-card\")\n",
    "\n",
    "for pet_card in pet_cards[len(pet_cards)-34:]:\n",
    "    number = pet_card.find('div', class_='pet-title huiji-tt').get('data-name')\n",
    "    name = pet_card.find('div', class_='pet-name').text\n",
    "    image_url = pet_card.find('img', alt=True)\n",
    "    high_res_url = image_url['srcset'].split(' ')[0]\n",
    "    urllib.request.urlretrieve(high_res_url, f'images_{year}/No.{number}_{name}.png')\n",
    "    print(f\"download: No.{number}_{name}.png\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**1.4 SEER**\n",
    "\n",
    "1.4.1 Crawling data from 4399"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import ElementNotInteractableException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import time\n",
    "# Terminal: /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222\n",
    "def create_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_experimental_option(\"debuggerAddress\", \"127.0.0.1:9222\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "url = 'https://news.4399.com/seer/jinglingdaquan/'\n",
    "driver = create_driver()\n",
    "driver.get(url)\n",
    "\n",
    "mode_button=driver.find_element(By.XPATH,'//*[@id=\"tab4\"]/li[2]/a')\n",
    "mode_button.click()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        more_button=driver.find_element(By.XPATH,'/html/body/div[6]/div[5]/a')\n",
    "        more_button.click()\n",
    "        time.sleep(1)\n",
    "    except ElementNotInteractableException:\n",
    "        break\n",
    "\n",
    "pet_cards=driver.find_elements(By.CLASS_NAME,'jl_list2')\n",
    "pet_cards = pet_cards[0].text\n",
    "pet_cards = pet_cards.split('\\n')\n",
    "pet_cards = [x for x in pet_cards if not ('形态等级'in x or '种族值'in x or '专属技能'in x or '查询捕捉'in x or '详细信息' in x)]\n",
    "#print(pet_cards)\n",
    "names = []\n",
    "numbers = []\n",
    "elements = []\n",
    "\n",
    "for index, info in enumerate(pet_cards):\n",
    "    if index % 3 == 0 and info: \n",
    "        names.append(info)\n",
    "    elif index % 3 == 1 and '精灵ID' in info:\n",
    "        number = info.split('：')[-1]\n",
    "        numbers.append(number)\n",
    "    elif index % 3 == 2 and '精灵属性' in info:\n",
    "        element = info.split('：')[-1]\n",
    "        elements.append(element)\n",
    "\n",
    "link_urls = []\n",
    "i=1\n",
    "while True:\n",
    "    try:\n",
    "        card_link=driver.find_element(By.XPATH,f'/html/body/div[6]/ul[2]/li[{i}]/a')\n",
    "        link_url = card_link.get_attribute('href')\n",
    "        link_urls.append(link_url)\n",
    "        i+=1\n",
    "    except NoSuchElementException:\n",
    "        break\n",
    "df = pd.DataFrame({'No.': [int(number) for number in numbers],\n",
    "                   'Name': names,\n",
    "                   'Elements': elements,\n",
    "                   'URL':link_urls})\n",
    "df_sorted = df.sort_values(by='No.', ascending=True)\n",
    "print(df_sorted)\n",
    "csv_file_path = 'seer_data_4399.txt'\n",
    "df_sorted.to_csv(csv_file_path, index=False,sep='\\t', encoding='utf-8') "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "url = 'https://news.4399.com/seer/jinglingdaquan/'\n",
    "\n",
    "# Terminal: /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222\n",
    "def create_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_experimental_option(\"debuggerAddress\", \"127.0.0.1:9222\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "driver = create_driver()\n",
    "driver.get(url)\n",
    "def process_link(driver, index):\n",
    "    li_xpath = f'//*[@id=\"state\"]/div[1]/ul/li[{index}]'\n",
    "    time.sleep(1)\n",
    "    li_element = driver.find_element(By.XPATH, li_xpath)\n",
    "    span_element = None\n",
    "    try:\n",
    "        span_element = li_element.find_element(By.XPATH, './/span')\n",
    "    except NoSuchElementException:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    img_xpath = f'//*[@id=\"state\"]/div[1]/ul/li[{index}]/a/img'\n",
    "    time.sleep(1)\n",
    "    img = driver.find_element(By.XPATH, img_xpath)\n",
    "    pic = img.get_attribute('src')\n",
    "    img.click()\n",
    "    number = driver.find_element(By.XPATH, f'//*[@id=\"state\"]/div[2]/dl[{index}]/dd/span[1]').text.replace('精灵序号：', '')\n",
    "    name = driver.find_element(By.XPATH, f'//*[@id=\"state\"]/div[2]/dl[{index}]/dt').text\n",
    "    element = driver.find_element(By.XPATH, f'//*[@id=\"state\"]/div[3]/dl[{index}]/dt/i').text\n",
    "    return name, number, element, pic\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "names = []\n",
    "numbers = []\n",
    "elements = []\n",
    "pics = []\n",
    "\n",
    "data=pd.read_csv('seer_data_4399.txt', sep='\\t')\n",
    "link_urls = data['URL']\n",
    "link_urls = link_urls[665:]\n",
    "for link_url in link_urls:\n",
    "    driver.get(link_url)\n",
    "    \n",
    "    for index in range(1,4):\n",
    "        name, number, element, pic = process_link(driver, index)\n",
    "        if name and number and element: \n",
    "            names.append(name)\n",
    "            numbers.append(number)\n",
    "            elements.append(element)\n",
    "            pics.append(pic)\n",
    "            print(f'{number}, {name}, {element}, {pic}')\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "df = pd.DataFrame({'No.': [int(number) for number in numbers],\n",
    "                   'Name': names,\n",
    "                   'Elements': elements,\n",
    "                   'image_url': pics})\n",
    "\n",
    "df_sorted = df.sort_values(by='No.', ascending=True)\n",
    "print(df_sorted)\n",
    "csv_file_path = 'seer_data_4399_1.txt'\n",
    "df_sorted.to_csv(csv_file_path, index=False,sep='\\t', encoding='utf-8') \n",
    "\n",
    "df=pd.read_csv('seer_data_4399_1.txt',sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.4.2 Crawling data from wiki "
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "url = 'https://seerelvesinpokemonstyle.fandom.com/zh/wiki/%E7%B2%BE%E7%81%B5%E5%88%97%E8%A1%A8%EF%BC%88%E6%8C%89%E5%85%A8%E5%9F%9F%E5%9B%BE%E9%89%B4%E7%BC%96%E5%8F%B7%EF%BC%89?variant=zh-cn'\n",
    "driver = create_driver()\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "table=driver.find_element(By.XPATH,'/html/body/div[6]/div[4]/div[4]/main/div[3]/div/div/table[1]')\n",
    "\n",
    "names = []\n",
    "numbers = []\n",
    "elements = []\n",
    "for i in range(1,192):\n",
    "    number=driver.find_element(By.XPATH,f'/html/body/div[6]/div[4]/div[4]/main/div[3]/div/div/table[1]/tbody/tr[{i}]/td[1]').text\n",
    "    \n",
    "    try:\n",
    "        name=driver.find_element(By.XPATH,f'/html/body/div[6]/div[4]/div[4]/main/div[3]/div/div/table[1]/tbody/tr[{i}]/td[3]/a').text\n",
    "    except NoSuchElementException:\n",
    "        try:\n",
    "            name=driver.find_element(By.XPATH,f'//*[@id=\"mw-content-text\"]/div/table[1]/tbody/tr[{i}]/td[3]/span').text\n",
    "        except NoSuchElementException:\n",
    "            name = None\n",
    "\n",
    "    ele = driver.find_element(By.XPATH, f'/html/body/div[6]/div[4]/div[4]/main/div[3]/div/div/table[1]/tbody/tr[{i}]/td[4]').text\n",
    "    ele2 = None  \n",
    "    try:\n",
    "        ele2 = driver.find_element(By.XPATH, f'/html/body/div[6]/div[4]/div[4]/main/div[3]/div/div/table[1]/tbody/tr[{i}]/td[5]').text\n",
    "    except NoSuchElementException:\n",
    "        pass \n",
    "\n",
    "    if ele2:  \n",
    "        ele = [ele, ele2]\n",
    "        ele = '，'.join(ele)  \n",
    "    else:\n",
    "        ele = ele\n",
    "    \n",
    "    # fig = driver.find_element(By.XPATH,f'/html/body/div[6]/div[4]/div[4]/main/div[3]/div/div/table[1]/tbody/tr[{i}]/td[2]/a')\n",
    "    # fig_url=fig.get_attribute('href')\n",
    "    # urllib.request.urlretrieve(fig_url, f'images/No.{number}_{name}.png')\n",
    "    # print(f'downlaod: No.{number}_{name}.png')\n",
    "    \n",
    "    names.append(name)\n",
    "    numbers.append(number)\n",
    "    elements.append(ele)\n",
    "\n",
    "df = pd.DataFrame({'No.': [int(number) for number in numbers],\n",
    "                   'Name': names,\n",
    "                   'Elements': elements})\n",
    "\n",
    "csv_file_path = 'seer_data_wiki.txt'\n",
    "df.to_csv(csv_file_path, index=False,sep='\\t', encoding='utf-8') \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.4.3 Translate Chinese into English"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from pypinyin import lazy_pinyin\n",
    "import pandas as pd\n",
    "directory_path = '/Users/fonglengsut/Desktop/introduction of ds/group project/other games/seer/images'\n",
    "rename_files(directory_path)\n",
    "def add_english_names_to_txt(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df=pd.read_csv(file_path,sep='\\t')\n",
    "            if 'Name' in df.columns:\n",
    "                df['Eng_Name'] = df['Name'].apply(lambda x: ''.join(lazy_pinyin(x)))\n",
    "                df.to_csv(file_path, index=False, sep=',')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "directory_path = '/Users/fonglengsut/Desktop/introduction of ds/group project/other games/seer'\n",
    "add_english_names_to_txt(directory_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<font size=\"6\">**Task 2: Training of pokemon type**</font>"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**2.1 Preprocessing**"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:22:46.406457Z",
     "start_time": "2024-12-13T07:22:46.395131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "\n",
    "# 定义自定义数据集类\n",
    "class PokemonDataset(Dataset):\n",
    "    def __init__(self, data_dict, transform=None):\n",
    "        self.data_dict = data_dict\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # 遍历数据字典，获取所有图像文件路径和对应的标签（属性）\n",
    "        for pokemon_name, info in data_dict.items():\n",
    "            for img_path in info['images']:\n",
    "                self.image_files.append(img_path)\n",
    "                self.labels.append(info['types'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # 获取宝可梦的标签（属性）\n",
    "        labels = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # 将字符串标签转换为整数标签\n",
    "        label_map = {'grass': 0, 'poison': 1, 'fire': 2, 'water': 3, 'electric': 4, 'ice': 5, 'fighting': 6,\n",
    "                     'ground': 7, 'flying': 8, 'psychic': 9, 'bug': 10, 'rock': 11, 'ghost': 12, 'dark': 13,\n",
    "                     'dragon': 14, 'steel': 15, 'fairy': 16, 'normal': 17}\n",
    "        labels = [label_map[label] for label in labels]\n",
    "        \n",
    "        # 将标签转换为多标签二进制格式\n",
    "        multi_label = torch.zeros(len(label_map), dtype=torch.float32)\n",
    "        for label in labels:\n",
    "            multi_label[label] = 1.0\n",
    "        \n",
    "        return image, multi_label\n",
    "\n",
    "# 读取数据字典\n",
    "with open('data/pokemon_info.json', 'r') as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "# 定义训练数据的转换\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 创建数据集\n",
    "pokemon_dataset = PokemonDataset(data_dict=data_dict, transform=data_transforms)\n",
    "\n",
    "# 将数据集分割为训练集、验证集和测试集（80%、10%、10%）\n",
    "train_size = int(0.8 * len(pokemon_dataset))\n",
    "val_size = int(0.1 * len(pokemon_dataset))\n",
    "test_size = len(pokemon_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(pokemon_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**2.2 Model and training**"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:32:01.469290Z",
     "start_time": "2024-12-13T07:24:02.166505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 定义自定义数据集类\n",
    "class PokemonDataset(Dataset):\n",
    "    def __init__(self, data_dict, transform=None):\n",
    "        self.data_dict = data_dict\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # 遍历数据字典，获取所有图像文件路径和对应的标签（属性）\n",
    "        for pokemon_name, info in data_dict.items():\n",
    "            for img_path in info['images']:\n",
    "                self.image_files.append(img_path)\n",
    "                self.labels.append(info['types'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        \n",
    "        # 使用OpenCV加载图像\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            print(f\"UnidentifiedImageError: cannot identify image file '{img_path}'\")\n",
    "            return self.__getitem__((idx + 1) % len(self.image_files))\n",
    "        \n",
    "        # 将图像从BGR转换为RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 获取宝可梦的标签（属性）\n",
    "        labels = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # 将字符串标签转换为整数标签\n",
    "        label_map = {'grass': 0, 'poison': 1, 'fire': 2, 'water': 3, 'electric': 4, 'ice': 5, 'fighting': 6,\n",
    "                     'ground': 7, 'flying': 8, 'psychic': 9, 'bug': 10, 'rock': 11, 'ghost': 12, 'dark': 13,\n",
    "                     'dragon': 14, 'steel': 15, 'fairy': 16, 'normal': 17}\n",
    "        labels = [label_map[label] for label in labels]\n",
    "        \n",
    "        # 将标签转换为多标签二进制格式\n",
    "        multi_label = torch.zeros(len(label_map), dtype=torch.float32)\n",
    "        for label in labels:\n",
    "            multi_label[label] = 1.0\n",
    "        \n",
    "        return image, multi_label\n",
    "\n",
    "# 定义函数来提取图像特征\n",
    "def extract_features(model, dataloader):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            features.append(outputs.cpu().numpy())\n",
    "    return np.concatenate(features)\n",
    "\n",
    "\n",
    "# 读取数据字典\n",
    "with open('data/pokemon_info.json', 'r') as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "# 定义训练数据的转换\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 创建数据集\n",
    "pokemon_dataset = PokemonDataset(data_dict=data_dict, transform=data_transforms)\n",
    "\n",
    "# 将数据集分割为训练集、验证集和测试集（80%、10%、10%）\n",
    "train_size = int(0.8 * len(pokemon_dataset))\n",
    "val_size = int(0.1 * len(pokemon_dataset))\n",
    "test_size = len(pokemon_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(pokemon_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 加载预训练的ResNet18模型\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# 修改最后一层以匹配属性的数量（假设有18种可能的属性）\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 18)\n",
    "\n",
    "# 将模型移动到GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# 训练循环\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs_labels in train_loader:\n",
    "        if inputs_labels is None:\n",
    "            continue\n",
    "        \n",
    "        inputs, labels = inputs_labels\n",
    "        \n",
    "        # 将输入和标签移动到GPU\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # 验证循环\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs_labels in val_loader:\n",
    "            if inputs_labels is None:\n",
    "                continue\n",
    "            \n",
    "            inputs, labels = inputs_labels\n",
    "            \n",
    "            # 将输入和标签移动到GPU\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# 测试循环（可选）\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs_labels in test_loader:\n",
    "        if inputs_labels is None:\n",
    "            continue\n",
    "        \n",
    "        inputs, labels = inputs_labels\n",
    "        \n",
    "        # 将输入和标签移动到GPU\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch 1/20, Loss: 0.3015347325624271\n",
      "Validation Loss: 0.28118698222121014\n",
      "Epoch 2/20, Loss: 0.2769729947487925\n",
      "Validation Loss: 0.26360021900270403\n",
      "Epoch 3/20, Loss: 0.2636679448959432\n",
      "Validation Loss: 0.2527120541205111\n",
      "Epoch 4/20, Loss: 0.25308770371765055\n",
      "Validation Loss: 0.2441973531829942\n",
      "Epoch 5/20, Loss: 0.24287912857578825\n",
      "Validation Loss: 0.2363066161723481\n",
      "Epoch 6/20, Loss: 0.23445101987589825\n",
      "Validation Loss: 0.2303259541693422\n",
      "Epoch 7/20, Loss: 0.22668955533963103\n",
      "Validation Loss: 0.22565625984336912\n",
      "Epoch 8/20, Loss: 0.21940744281239768\n",
      "Validation Loss: 0.2215224751799377\n",
      "Epoch 9/20, Loss: 0.20787746508465837\n",
      "Validation Loss: 0.21708014538300405\n",
      "Epoch 10/20, Loss: 0.19980790511429927\n",
      "Validation Loss: 0.2146109573005401\n",
      "Epoch 11/20, Loss: 0.19193015428057297\n",
      "Validation Loss: 0.21295362611099616\n",
      "Epoch 12/20, Loss: 0.1812104575741322\n",
      "Validation Loss: 0.21135934019826122\n",
      "Epoch 13/20, Loss: 0.17058763386672024\n",
      "Validation Loss: 0.20369247996161893\n",
      "Epoch 14/20, Loss: 0.16265154782431732\n",
      "Validation Loss: 0.20995871907042474\n",
      "Epoch 15/20, Loss: 0.15261471620881495\n",
      "Validation Loss: 0.2030828278703788\n",
      "Epoch 16/20, Loss: 0.1417701702873787\n",
      "Validation Loss: 0.2014562721104966\n",
      "Epoch 17/20, Loss: 0.1321228461513788\n",
      "Validation Loss: 0.20103522678165092\n",
      "Epoch 18/20, Loss: 0.12329844008513528\n",
      "Validation Loss: 0.20027692888661758\n",
      "Epoch 19/20, Loss: 0.11275986282376892\n",
      "Validation Loss: 0.19677516879494658\n",
      "Epoch 20/20, Loss: 0.10482501660816256\n",
      "Validation Loss: 0.20139268113626646\n",
      "Training complete.\n",
      "Test Loss: 0.20307834438749195\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<font size=\"6\">**Task 3: Analysis of types from other games**</font>"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<font size=\"6\">**Task 4: Most similar pokemon**</font>"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**4.1 Different functions of calculation of similarity**\n",
    "\n",
    "4.1.1 cosine_similarity"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:32:33.714860Z",
     "start_time": "2024-12-13T07:32:13.356225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 示例：查找与查询图像最相似的三个宝可梦图像\n",
    "# 定义函数来找到最相似的宝可梦图像\n",
    "def find_similar_images(query_image_path, model, feature_extractor, dataset, dataloader, top_k=3):\n",
    "    # 加载查询图像并进行预处理\n",
    "    query_image = cv2.imread(query_image_path)\n",
    "    if query_image is None:\n",
    "        raise ValueError(f\"Cannot identify image file '{query_image_path}'\")\n",
    "    \n",
    "    query_image = cv2.cvtColor(query_image, cv2.COLOR_BGR2RGB)\n",
    "    query_image = data_transforms(query_image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 提取查询图像的特征\n",
    "    with torch.no_grad():\n",
    "        query_features = feature_extractor(query_image).cpu().numpy()\n",
    "    \n",
    "    # 提取数据集中所有图像的特征\n",
    "    dataset_features = extract_features(feature_extractor, dataloader)\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    similarities = cosine_similarity(query_features, dataset_features)\n",
    "    \n",
    "    # 找到最相似的图像索引\n",
    "    top_k_indices = np.argsort(similarities[0])[::-1][:top_k]\n",
    "    \n",
    "    # 返回最相似的图像路径和相似度分数\n",
    "    similar_images = [(dataset.image_files[idx], similarities[0][idx]) for idx in top_k_indices]\n",
    "    \n",
    "    return similar_images\n",
    "\n",
    "query_image_path = 'data/palworld_images/Anubis.png'\n",
    "similar_images = find_similar_images(query_image_path, model, model, pokemon_dataset, train_loader, top_k=3)\n",
    "print(\"Most similar images:\")\n",
    "for img_path, score in similar_images:\n",
    "    print(f\"Image: {img_path}, Similarity Score: {score}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar images:\n",
      "Image: data/pokemon_images\\omanyte_pic1.png, Similarity Score: 0.9607487916946411\n",
      "Image: data/pokemon_images\\roserade_pic2.png, Similarity Score: 0.9476977586746216\n",
      "Image: data/pokemon_images\\fraxure_pic1.png, Similarity Score: 0.9373520612716675\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4.1.2 Euclidean"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:44:03.856273Z",
     "start_time": "2024-12-13T07:43:44.288476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def find_similar_images_euclidean(query_image_path, model, feature_extractor, dataset, dataloader, top_k=3):\n",
    "    query_image = cv2.imread(query_image_path)\n",
    "    if query_image is None:\n",
    "        raise ValueError(f\"Cannot identify image file '{query_image_path}'\")\n",
    "\n",
    "    query_image = cv2.cvtColor(query_image, cv2.COLOR_BGR2RGB)\n",
    "    query_image = data_transforms(query_image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        query_features = feature_extractor(query_image).cpu().numpy()\n",
    "\n",
    "    dataset_features = extract_features(feature_extractor, dataloader)\n",
    "\n",
    "    distances = euclidean_distances(query_features, dataset_features)\n",
    "\n",
    "    top_k_indices = np.argsort(distances[0])[:top_k]\n",
    "\n",
    "    similar_images = [(dataset.image_files[idx], distances[0][idx]) for idx in top_k_indices]\n",
    "\n",
    "    return similar_images\n",
    "\n",
    "query_image_path = 'data/palworld_images/Anubis.png'\n",
    "similar_images = find_similar_images_euclidean(query_image_path, model, model, pokemon_dataset, train_loader, top_k=3)\n",
    "print(\"Most similar images:\")\n",
    "for img_path, score in similar_images:\n",
    "    print(f\"Image: {img_path}, Distance: {score}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar images:\n",
      "Image: data/pokemon_images\\nidoqueen_pic2.png, Distance: 9.441254615783691\n",
      "Image: data/pokemon_images\\toxapex_pic2.png, Distance: 9.872849464416504\n",
      "Image: data/pokemon_images\\kirlia_pic3.png, Distance: 10.014799118041992\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4.1.3 VGG"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:34:34.591012Z",
     "start_time": "2024-12-13T07:34:10.233796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.models import vgg16\n",
    "\n",
    "# 加载预训练的VGG16模型\n",
    "vgg_model = vgg16(pretrained=True)\n",
    "vgg_model.classifier = nn.Sequential(*list(vgg_model.classifier.children())[:-1])  # 移除最后一层\n",
    "vgg_model = vgg_model.to(device)\n",
    "\n",
    "def extract_vgg_features(model, dataloader):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            features.append(outputs.cpu().numpy())\n",
    "    return np.concatenate(features)\n",
    "\n",
    "query_image_path = 'data/palworld_images/Anubis.png'\n",
    "similar_images = find_similar_images(query_image_path, vgg_model, vgg_model, pokemon_dataset, train_loader, top_k=3)\n",
    "print(\"Most similar images:\")\n",
    "for img_path, score in similar_images:\n",
    "    print(f\"Image: {img_path}, Similarity Score: {score}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\业\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\业\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar images:\n",
      "Image: data/pokemon_images\\inkay_pic3.png, Similarity Score: 0.46852004528045654\n",
      "Image: data/pokemon_images\\simipour_pic1.png, Similarity Score: 0.4677070379257202\n",
      "Image: data/pokemon_images\\koraidon_pic2.png, Similarity Score: 0.45726901292800903\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4.1.4 Hashes"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:35:15.985748Z",
     "start_time": "2024-12-13T07:35:06.936753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import imagehash\n",
    "import numpy as np\n",
    " \n",
    "def find_similar_images_hash(query_image_path, dataset, top_k=3):\n",
    "    # 使用 OpenCV 读取图像\n",
    "    query_image = cv2.imread(query_image_path)\n",
    "    # OpenCV 读取的图像是 BGR 格式，需要转换为 RGB 格式\n",
    "    query_image_rgb = cv2.cvtColor(query_image, cv2.COLOR_BGR2RGB)\n",
    "    # 将图像转换为 Pillow 图像对象以使用 imagehash 库\n",
    "    query_image_pil = Image.fromarray(query_image_rgb)\n",
    "    query_hash = imagehash.phash(query_image_pil)\n",
    " \n",
    "    image_hashes = []\n",
    "    for img_path in dataset.image_files:\n",
    "        # 使用 OpenCV 读取图像\n",
    "        img = cv2.imread(img_path)\n",
    "        # 转换为 RGB 格式\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # 转换为 Pillow 图像对象\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        # 计算图像哈希\n",
    "        img_hash = imagehash.phash(img_pil)\n",
    "        image_hashes.append((img_path, img_hash))\n",
    " \n",
    "    similarities = [(img_path, 1 - (query_hash - img_hash) / len(query_hash.hash)**2) for img_path, img_hash in image_hashes]\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    " \n",
    "    return similarities[:top_k]\n",
    " \n",
    "query_image_path = 'data/palworld_images/Anubis.png'\n",
    "# 假设 pokemon_dataset 是一个具有 image_files 属性的对象，该属性包含图像文件路径的列表\n",
    "# 注意：你需要确保 pokemon_dataset 已经被正确定义并包含有效的图像文件路径\n",
    "similar_images = find_similar_images_hash(query_image_path, pokemon_dataset, top_k=3)\n",
    "print(\"Most similar images:\")\n",
    "for img_path, score in similar_images:\n",
    "    print(f\"Image: {img_path}, Similarity Score: {score}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar images:\n",
      "Image: data/pokemon_images\\pichu_pic3.png, Similarity Score: 0.75\n",
      "Image: data/pokemon_images\\chinchou_pic3.png, Similarity Score: 0.71875\n",
      "Image: data/pokemon_images\\porygon2_pic2.png, Similarity Score: 0.71875\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4.1.5 Combined"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:56:15.032906Z",
     "start_time": "2024-12-13T07:55:46.382288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "def find_similar_images_combined(query_image_path, model, feature_extractor, dataset, dataloader, top_k=3):\n",
    "    # 加载查询图像并进行预处理\n",
    "    query_image = cv2.imread(query_image_path)\n",
    "    if query_image is None:\n",
    "        raise ValueError(f\"Cannot identify image file '{query_image_path}'\")\n",
    "    query_image = cv2.cvtColor(query_image, cv2.COLOR_BGR2RGB)\n",
    "    query_image_tensor = data_transforms(query_image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 提取查询图像的特征\n",
    "    with torch.no_grad():\n",
    "        query_features = feature_extractor(query_image_tensor).cpu().numpy()\n",
    "    \n",
    "    # 提取数据集中所有图像的特征\n",
    "    dataset_features = extract_features(feature_extractor, dataloader)\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    cosine_similarities = cosine_similarity(query_features, dataset_features)\n",
    "    \n",
    "    # 计算哈希相似度\n",
    "    query_image_pil = Image.fromarray(query_image)\n",
    "    query_hash = imagehash.phash(query_image_pil)\n",
    "    image_hashes = []\n",
    "    for img_path in dataset.image_files:\n",
    "        img = cv2.imread(img_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        img_hash = imagehash.phash(img_pil)\n",
    "        image_hashes.append((img_path, img_hash))\n",
    "    \n",
    "    # 计算哈希相似度的归一化分数（这里使用汉明距离的平方的倒数进行归一化）\n",
    "    # 注意：由于phash产生的是64位的哈希值，因此最大可能的汉明距离是64\n",
    "    max_hamming_distance = 64\n",
    "    hash_similarities = []\n",
    "    for img_path, img_hash in image_hashes:\n",
    "        hamming_distance = query_hash - img_hash  # imagehash库支持直接相减得到汉明距离\n",
    "        normalized_hash_similarity = 1 - (hamming_distance / max_hamming_distance)  # 归一化到[0, 1]\n",
    "        hash_similarities.append((img_path, normalized_hash_similarity))\n",
    "    \n",
    "    # 创建一个字典来存储所有图像的路径和它们的加权相似度分数\n",
    "    final_scores = {}\n",
    "    for i, (img_path_cosine, cosine_score) in enumerate(zip(dataset.image_files, cosine_similarities[0])):\n",
    "        # 由于hash_similarities也是按照dataset.image_files的顺序，因此可以直接用i索引\n",
    "        img_path_hash, hash_score = hash_similarities[i]\n",
    "        # 确保两者对应的是同一张图像（理论上应该是，但为了安全起见还是检查一下）\n",
    "        assert img_path_cosine == img_path_hash, \"Image paths mismatch between cosine and hash similarities\"\n",
    "        # 计算加权相似度分数\n",
    "        final_score = 0.8 * cosine_score + 0.2 * hash_score\n",
    "        final_scores[img_path_cosine] = final_score\n",
    "    \n",
    "    # 根据最终相似度分数排序，返回前三名\n",
    "    sorted_final_scores = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_final_scores[:top_k]\n",
    "\n",
    "query_image_path = 'data/palworld_images/Anubis.png'\n",
    "# 假设 pokemon_dataset 是一个具有 image_files 属性的对象，该属性包含图像文件路径的列表\n",
    "# 注意：你需要确保 pokemon_dataset 已经被正确定义并包含有效的图像文件路径\n",
    "similar_images = find_similar_images_combined(query_image_path, model, model, pokemon_dataset, train_loader, top_k=3)\n",
    "print(\"Most similar images:\")\n",
    "for img_path, score in similar_images:\n",
    "    print(f\"Image: {img_path}, Similarity Score: {score}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar images:\n",
      "Image: data/pokemon_images\\whirlipede_pic3.png, Similarity Score: 0.8699132442474367\n",
      "Image: data/pokemon_images\\rellor_pic3.png, Similarity Score: 0.8518013000488283\n",
      "Image: data/pokemon_images\\caterpie_pic3.png, Similarity Score: 0.8471257209777833\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**4.2  Try to use unet model**"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T06:45:24.481033Z",
     "start_time": "2024-12-13T06:45:23.008893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, n_classes, kernel_size=3, padding=1),\n",
    "        )\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # 全局平均池化\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        mid = self.middle(enc)\n",
    "        dec = self.decoder(mid)\n",
    "        out = self.global_avg_pool(dec)  # 形状变为 [batch_size, n_classes, 1, 1]\n",
    "        return out.view(out.size(0), -1)  # 形状变为 [batch_size, n_classes]\n",
    "\n",
    "\n",
    "# 初始化 UNet 模型\n",
    "n_channels = 3  # 输入图像通道数（RGB 图像）\n",
    "n_classes = 18  # 输出类别数（多标签分类）\n",
    "model_u = UNet(n_channels, n_classes)\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model_u = model_u.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCEWithLogitsLoss()  # 多标签分类任务的损失函数\n",
    "optimizer = optim.Adam(model_u.parameters(), lr=0.001)  # 使用 Adam 优化器"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:15:11.090378Z",
     "start_time": "2024-12-13T06:54:45.869318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image, ImageFile, UnidentifiedImageError\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "num_epochs = 40\n",
    "for epoch in range(num_epochs):\n",
    "    model_u.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs_labels in train_loader:\n",
    "        if inputs_labels is None:\n",
    "            continue\n",
    "        \n",
    "        inputs, labels = inputs_labels\n",
    "        \n",
    "        # 将输入和标签移动到 GPU\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_u(inputs)\n",
    "\n",
    "        # 将输出调整为与标签形状一致\n",
    "        outputs = outputs.view(outputs.size(0), -1)\n",
    "        labels = labels.view(labels.size(0), -1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # 验证循环\n",
    "    model_u.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs_labels in val_loader:\n",
    "            if inputs_labels is None:\n",
    "                continue\n",
    "\n",
    "            inputs, labels = inputs_labels\n",
    "\n",
    "            # 将输入和标签移动到 GPU\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model_u(inputs)\n",
    "\n",
    "            # 将输出调整为与标签形状一致\n",
    "            outputs = outputs.view(outputs.size(0), -1)\n",
    "            labels = labels.view(labels.size(0), -1)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 0.29572675599103765\n",
      "Validation Loss: 0.2938861498205932\n",
      "Epoch 2/40, Loss: 0.29342376415670846\n",
      "Validation Loss: 0.29187320940887806\n",
      "Epoch 3/40, Loss: 0.29265110920439114\n",
      "Validation Loss: 0.29155574892599556\n",
      "Epoch 4/40, Loss: 0.29261692968520475\n",
      "Validation Loss: 0.2922781739652771\n",
      "Epoch 5/40, Loss: 0.29270499404230266\n",
      "Validation Loss: 0.29135680367651673\n",
      "Epoch 6/40, Loss: 0.2923475724350603\n",
      "Validation Loss: 0.29023011382093133\n",
      "Epoch 7/40, Loss: 0.29211469066034945\n",
      "Validation Loss: 0.2907717083532786\n",
      "Epoch 8/40, Loss: 0.29219488403880534\n",
      "Validation Loss: 0.2894417121238315\n",
      "Epoch 9/40, Loss: 0.2914232803711311\n",
      "Validation Loss: 0.28792982202829775\n",
      "Epoch 10/40, Loss: 0.2907567130006039\n",
      "Validation Loss: 0.288155108075781\n",
      "Epoch 11/40, Loss: 0.29009782461150324\n",
      "Validation Loss: 0.28611178988033964\n",
      "Epoch 12/40, Loss: 0.2889221167201514\n",
      "Validation Loss: 0.28576364544863553\n",
      "Epoch 13/40, Loss: 0.2884591786586559\n",
      "Validation Loss: 0.28445007982327764\n",
      "Epoch 14/40, Loss: 0.28732194460040544\n",
      "Validation Loss: 0.2861774159768193\n",
      "Epoch 15/40, Loss: 0.283882427786916\n",
      "Validation Loss: 0.27732104877221214\n",
      "Epoch 16/40, Loss: 0.2804991721605081\n",
      "Validation Loss: 0.27545302461103066\n",
      "Epoch 17/40, Loss: 0.2787053816869778\n",
      "Validation Loss: 0.27402479639372873\n",
      "Epoch 18/40, Loss: 0.27680726515351184\n",
      "Validation Loss: 0.27512961195916247\n",
      "Epoch 19/40, Loss: 0.27419393656321756\n",
      "Validation Loss: 0.27308470978564825\n",
      "Epoch 20/40, Loss: 0.2726688857909311\n",
      "Validation Loss: 0.26846485131794645\n",
      "Epoch 21/40, Loss: 0.2709844179953318\n",
      "Validation Loss: 0.2693520764407423\n",
      "Epoch 22/40, Loss: 0.27032382710030967\n",
      "Validation Loss: 0.2691262410473578\n",
      "Epoch 23/40, Loss: 0.2702601843470119\n",
      "Validation Loss: 0.26870004417970006\n",
      "Epoch 24/40, Loss: 0.26826268652637386\n",
      "Validation Loss: 0.26888986392733977\n",
      "Epoch 25/40, Loss: 0.26755428152071997\n",
      "Validation Loss: 0.26570360669770193\n",
      "Epoch 26/40, Loss: 0.26572950149601604\n",
      "Validation Loss: 0.2611859419296697\n",
      "Epoch 27/40, Loss: 0.26309181560660894\n",
      "Validation Loss: 0.26295947367997513\n",
      "Epoch 28/40, Loss: 0.2621617164617221\n",
      "Validation Loss: 0.26195521913852887\n",
      "Epoch 29/40, Loss: 0.2595774527697057\n",
      "Validation Loss: 0.25888233363013907\n",
      "Epoch 30/40, Loss: 0.2572686614594157\n",
      "Validation Loss: 0.26639750123638467\n",
      "Epoch 31/40, Loss: 0.25580839440226555\n",
      "Validation Loss: 0.25594190285377894\n",
      "Epoch 32/40, Loss: 0.25426877205081555\n",
      "Validation Loss: 0.2578146913924168\n",
      "Epoch 33/40, Loss: 0.25185738091834775\n",
      "Validation Loss: 0.25639392312654513\n",
      "Epoch 34/40, Loss: 0.25049485629086665\n",
      "Validation Loss: 0.2575528758395578\n",
      "Epoch 35/40, Loss: 0.24952182056951708\n",
      "Validation Loss: 0.2522827737110177\n",
      "Epoch 36/40, Loss: 0.24739120005982218\n",
      "Validation Loss: 0.24944299152216962\n",
      "Epoch 37/40, Loss: 0.24413987735546933\n",
      "Validation Loss: 0.25365609621878754\n",
      "Epoch 38/40, Loss: 0.241841937149899\n",
      "Validation Loss: 0.2507902188706644\n",
      "Epoch 39/40, Loss: 0.23985760171603354\n",
      "Validation Loss: 0.25038478039589124\n",
      "Epoch 40/40, Loss: 0.23645372899162337\n",
      "Validation Loss: 0.2546477270187791\n",
      "Training complete.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:57:40.508089Z",
     "start_time": "2024-12-13T07:57:13.893736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_image_path = 'data/palworld_images/Anubis.png'\n",
    "# 假设 pokemon_dataset 是一个具有 image_files 属性的对象，该属性包含图像文件路径的列表\n",
    "# 注意：你需要确保 pokemon_dataset 已经被正确定义并包含有效的图像文件路径\n",
    "similar_images = find_similar_images_combined(query_image_path, model_u, model_u, pokemon_dataset, train_loader, top_k=3)\n",
    "print(\"Most similar images:\")\n",
    "for img_path, score in similar_images:\n",
    "    print(f\"Image: {img_path}, Similarity Score: {score}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar images:\n",
      "Image: data/pokemon_images\\tsareena_pic1.png, Similarity Score: 0.7474530220031739\n",
      "Image: data/pokemon_images\\meditite_pic1.png, Similarity Score: 0.7121516704559326\n",
      "Image: data/pokemon_images\\croconaw_pic3.png, Similarity Score: 0.7039288520812989\n"
     ]
    }
   ],
   "execution_count": 43
  }
 ]
}
